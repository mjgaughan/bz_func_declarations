func_prototype,est_line,file,in_macro,a0,a1,a2,a3,a4,a5,a6,a7,a8,a9,immutable,label_time,func_body_text
"
int ubi_vtbl_rename_volumes(struct ubi_device *ubi,
			    struct list_head *rename_list);",841,../linux/drivers/mtd/ubi/ubi.h,False,struct ubi_device *ubi,ignore,u,u,u,u,u,u,u,u,True,0:00:01.210646,"			    struct list_head *rename_list)
{
	struct ubi_rename_entry *re;

	list_for_each_entry(re, rename_list, list) {
		uint32_t crc;
		struct ubi_volume *vol = re->desc->vol;
		struct ubi_vtbl_record *vtbl_rec = &ubi->vtbl[vol->vol_id];

		if (re->remove) {
			memcpy(vtbl_rec, &empty_vtbl_record,
			       sizeof(struct ubi_vtbl_record));
			continue;
		}

		vtbl_rec->name_len = cpu_to_be16(re->new_name_len);
		memcpy(vtbl_rec->name, re->new_name, re->new_name_len);
		memset(vtbl_rec->name + re->new_name_len, 0,
		       UBI_VOL_NAME_MAX + 1 - re->new_name_len);
		crc = crc32(UBI_CRC32_INIT, vtbl_rec,
			    UBI_VTBL_RECORD_SIZE_CRC);
		vtbl_rec->crc = cpu_to_be32(crc);
	}

	return ubi_update_layout_vol(ubi);
}
"
"
void vmw_kms_lost_device(struct drm_device *dev);",1215,../linux/drivers/gpu/drm/vmwgfx/vmwgfx_drv.h,False,struct drm_device *dev,u,u,u,u,u,u,u,u,u,True,0:00:01.658718,"{
	drm_atomic_helper_shutdown(dev);
}
"
"
void tcp_select_initial_window(const struct sock *sk, int __space,
			       __u32 mss, __u32 *rcv_wnd,
			       __u32 *window_clamp, int wscale_ok,
			       __u8 *rcv_wscale, __u32 init_rcv_wnd);",1401,../linux/include/net/tcp.h,False,ignore, int __space,ignore,ignore,ignore,ignore,ignore,ignore,u,u,False,0:00:01.326564,"			       __u32 *rcv_wnd, __u32 *window_clamp,
			       int wscale_ok, __u8 *rcv_wscale,
			       __u32 init_rcv_wnd)
{
	unsigned int space = (__space < 0 ? 0 : __space);

	/* If no clamp set the clamp to the max possible scaled window */
	if (*window_clamp == 0)
		(*window_clamp) = (U16_MAX << TCP_MAX_WSCALE);
	space = min(*window_clamp, space);

	/* Quantize space offering to a multiple of mss if possible. */
	if (space > mss)
		space = rounddown(space, mss);

	/* NOTE: offering an initial window larger than 32767
	 * will break some buggy TCP stacks. If the admin tells us
	 * it is likely we could be speaking with such a buggy stack
	 * we will truncate our initial window offering to 32K-1
	 * unless the remote has sent us a window scaling option,
	 * which we interpret as a sign the remote TCP is not
	 * misinterpreting the window field as a signed quantity.
	 */
	if (sock_net(sk)->ipv4.sysctl_tcp_workaround_signed_windows)
		(*rcv_wnd) = min(space, MAX_TCP_WINDOW);
	else
		(*rcv_wnd) = min_t(u32, space, U16_MAX);

	if (init_rcv_wnd)
		*rcv_wnd = min(*rcv_wnd, init_rcv_wnd * mss);

	*rcv_wscale = 0;
	if (wscale_ok) {
		/* Set window scaling on max possible window */
		space = max_t(u32, space, sock_net(sk)->ipv4.sysctl_tcp_rmem[2]);
		space = max_t(u32, space, sysctl_rmem_max);
		space = min_t(u32, space, *window_clamp);
		*rcv_wscale = clamp_t(int, ilog2(space) - 15,
				      0, TCP_MAX_WSCALE);
	}
	/* Set the clamp no higher than max representable value */
	(*window_clamp) = min_t(__u32, U16_MAX << (*rcv_wscale), *window_clamp);
}
"
"
bool zone_watermark_ok(struct zone *z, unsigned int order,
		unsigned long mark, int highest_zoneidx,
		unsigned int alloc_flags);",952,../linux/include/linux/mmzone.h,False,ignore, unsigned int order,ignore,ignore,ignore,u,u,u,u,u,True,0:00:03.034677,"			 int highest_zoneidx, unsigned int alloc_flags,
			 long free_pages)
{
	long min = mark;
	int o;
	const bool alloc_harder = (alloc_flags & (ALLOC_HARDER|ALLOC_OOM));

	/* free_pages may go negative - that's OK */
	free_pages -= __zone_watermark_unusable_free(z, order, alloc_flags);

	if (alloc_flags & ALLOC_HIGH)
		min -= min / 2;

	if (unlikely(alloc_harder)) {
		/*
		 * OOM victims can try even harder than normal ALLOC_HARDER
		 * users on the grounds that it's definitely going to be in
		 * the exit path shortly and free memory. Any allocation it
		 * makes during the free path will be small and short-lived.
		 */
		if (alloc_flags & ALLOC_OOM)
			min -= min / 2;
		else
			min -= min / 4;
	}

	/*
	 * Check watermarks for an order-0 allocation request. If these
	 * are not met, then a high-order request also cannot go ahead
	 * even if a suitable page happened to be free.
	 */
	if (free_pages <= min + z->lowmem_reserve[highest_zoneidx])
		return false;

	/* If this is an order-0 request then the watermark is fine */
	if (!order)
		return true;

	/* For a high-order request, check at least one suitable page is free */
	for (o = order; o < MAX_ORDER; o++) {
		struct free_area *area = &z->free_area[o];
		int mt;

		if (!area->nr_free)
			continue;

		for (mt = 0; mt < MIGRATE_PCPTYPES; mt++) {
			if (!free_area_empty(area, mt))
				return true;
		}

#ifdef CONFIG_CMA
		if ((alloc_flags & ALLOC_CMA) &&
		    !free_area_empty(area, MIGRATE_CMA)) {
			return true;
		}
#endif
		if (alloc_harder && !free_area_empty(area, MIGRATE_HIGHATOMIC))
			return true;
	}
	return false;
}
"
"
void udp_v6_rehash(struct sock *sk);",15,../linux/net/ipv6/udp_impl.h,False,struct sock *sk,u,u,u,u,u,u,u,u,u,False,0:00:01.311316,"{
	u16 new_hash = ipv6_portaddr_hash(sock_net(sk),
					  &sk->sk_v6_rcv_saddr,
					  inet_sk(sk)->inet_num);

	udp_lib_rehash(sk, new_hash);
}
"
"int comedi_pcmcia_enable(struct comedi_device *dev,
			 int (*conf_check)(struct pcmcia_device *p_dev,
					   void *priv_data));",21,../linux/drivers/comedi/comedi_pcmcia.h,False,struct comedi_device *dev,ignore,u,u,u,u,u,u,u,u,True,0:00:01.094932," * @dev: COMEDI device.
 * @conf_check: Optional callback to check each configuration option of the
 *	PCMCIA device and request I/O regions.
 *
 * Assuming @dev->hw_dev is non-%NULL, it is assumed to be pointing to a a
 * &struct device embedded in a &struct pcmcia_device.  The comedi PCMCIA
 * driver needs to set the 'config_flags' member in the &struct pcmcia_device,
 * as appropriate for that driver, before calling this function in order to
 * allow pcmcia_loop_config() to do its internal autoconfiguration.
 *
 * If @conf_check is %NULL it is set to a default function.  If is
 * passed to pcmcia_loop_config() and should return %0 if the configuration
 * is valid and I/O regions requested successfully, otherwise it should return
 * a negative error value.  The default function returns -%EINVAL if the
 * 'config_index' member is %0, otherwise it calls pcmcia_request_io() and
 * returns the result.
 *
 * If the above configuration check passes, pcmcia_enable_device() is called
 * to set up and activate the PCMCIA device.
 *
 * If this function returns an error, comedi_pcmcia_disable() should be called
 * to release requested resources.
 *
 * Return:
 *	0 on success,
 *	-%ENODEV id @dev->hw_dev is %NULL,
 *	a negative error number from pcmcia_loop_config() if it fails,
 *	or a negative error number from pcmcia_enable_device() if it fails.
 */
int comedi_pcmcia_enable(struct comedi_device *dev,
			 int (*conf_check)(struct pcmcia_device *p_dev,
					   void *priv_data))
{
	struct pcmcia_device *link = comedi_to_pcmcia_dev(dev);
	int ret;

	if (!link)
		return -ENODEV;

	if (!conf_check)
		conf_check = comedi_pcmcia_conf_check;

	ret = pcmcia_loop_config(link, conf_check, NULL);
	if (ret)
		return ret;

	return pcmcia_enable_device(link);
}
"
"irqreturn_t cqhci_irq(struct mmc_host *mmc, u32 intmask, int cmd_error,
		      int data_error);",313,../linux/drivers/mmc/host/cqhci.h,False,ignore, u32 intmask,ignore,ignore,u,u,u,u,u,u,True,0:00:01.359044,"		      int data_error)
{
	u32 status;
	unsigned long tag = 0, comp_status;
	struct cqhci_host *cq_host = mmc->cqe_private;

	status = cqhci_readl(cq_host, CQHCI_IS);
	cqhci_writel(cq_host, status, CQHCI_IS);

	pr_debug(""%s: cqhci: IRQ status: 0x%08x\n"", mmc_hostname(mmc), status);

	if ((status & (CQHCI_IS_RED | CQHCI_IS_GCE | CQHCI_IS_ICCE)) ||
	    cmd_error || data_error)
		cqhci_error_irq(mmc, status, cmd_error, data_error);

	if (status & CQHCI_IS_TCC) {
		/* read TCN and complete the request */
		comp_status = cqhci_readl(cq_host, CQHCI_TCN);
		cqhci_writel(cq_host, comp_status, CQHCI_TCN);
		pr_debug(""%s: cqhci: TCN: 0x%08lx\n"",
			 mmc_hostname(mmc), comp_status);

		spin_lock(&cq_host->lock);

		for_each_set_bit(tag, &comp_status, cq_host->num_slots) {
			/* complete the corresponding mrq */
			pr_debug(""%s: cqhci: completing tag %lu\n"",
				 mmc_hostname(mmc), tag);
			cqhci_finish_mrq(mmc, tag);
		}

		if (cq_host->waiting_for_idle && !cq_host->qcnt) {
			cq_host->waiting_for_idle = false;
			wake_up(&cq_host->wait_queue);
		}

		spin_unlock(&cq_host->lock);
	}

	if (status & CQHCI_IS_TCL)
		wake_up(&cq_host->wait_queue);

	if (status & CQHCI_IS_HAC)
		wake_up(&cq_host->wait_queue);

	return IRQ_HANDLED;
}
"
"
unsigned int aac_intr_normal(struct aac_dev *dev, u32 Index,
			int isAif, int isFastResponse,
			struct hw_fib *aif_fib);",2727,../linux/drivers/scsi/aacraid/aacraid.h,False,ignore,ignore,ignore, int isFastResponse,ignore,u,u,u,u,u,True,0:00:01.133370,"	int isFastResponse, struct hw_fib *aif_fib)
{
	unsigned long mflags;
	dprintk((KERN_INFO ""aac_intr_normal(%p,%x)\n"", dev, index));
	if (isAif == 1) {	/* AIF - common */
		struct hw_fib * hw_fib;
		struct fib * fib;
		struct aac_queue *q = &dev->queues->queue[HostNormCmdQueue];
		unsigned long flags;

		/*
		 *	Allocate a FIB. For non queued stuff we can just use
		 * the stack so we are happy. We need a fib object in order to
		 * manage the linked lists.
		 */
		if ((!dev->aif_thread)
		 || (!(fib = kzalloc(sizeof(struct fib),GFP_ATOMIC))))
			return 1;
		if (!(hw_fib = kzalloc(sizeof(struct hw_fib),GFP_ATOMIC))) {
			kfree (fib);
			return 1;
		}
		if (dev->sa_firmware) {
			fib->hbacmd_size = index;	/* store event type */
		} else if (aif_fib != NULL) {
			memcpy(hw_fib, aif_fib, sizeof(struct hw_fib));
		} else {
			memcpy(hw_fib, (struct hw_fib *)
				(((uintptr_t)(dev->regs.sa)) + index),
				sizeof(struct hw_fib));
		}
		INIT_LIST_HEAD(&fib->fiblink);
		fib->type = FSAFS_NTC_FIB_CONTEXT;
		fib->size = sizeof(struct fib);
		fib->hw_fib_va = hw_fib;
		fib->data = hw_fib->data;
		fib->dev = dev;
	
		spin_lock_irqsave(q->lock, flags);
		list_add_tail(&fib->fiblink, &q->cmdq);
	        wake_up_interruptible(&q->cmdready);
		spin_unlock_irqrestore(q->lock, flags);
		return 1;
	} else if (isAif == 2) {	/* AIF - new (SRC) */
		struct fib *fibctx;
		struct aac_aifcmd *cmd;

		fibctx = aac_fib_alloc(dev);
		if (!fibctx)
			return 1;
		aac_fib_init(fibctx);

		cmd = (struct aac_aifcmd *) fib_data(fibctx);
		cmd->command = cpu_to_le32(AifReqEvent);

		return aac_fib_send(AifRequest,
			fibctx,
			sizeof(struct hw_fib)-sizeof(struct aac_fibhdr),
			FsaNormal,
			0, 1,
			(fib_callback)aac_aif_callback, fibctx);
	} else {
		struct fib *fib = &dev->fibs[index];
		int start_callback = 0;

		/*
		 *	Remove this fib from the Outstanding I/O queue.
		 *	But only if it has not already been timed out.
		 *
		 *	If the fib has been timed out already, then just 
		 *	continue. The caller has already been notified that
		 *	the fib timed out.
		 */
		atomic_dec(&dev->queues->queue[AdapNormCmdQueue].numpending);

		if (unlikely(fib->flags & FIB_CONTEXT_FLAG_TIMED_OUT)) {
			aac_fib_complete(fib);
			aac_fib_free(fib);
			return 0;
		}

		FIB_COUNTER_INCREMENT(aac_config.FibRecved);

		if (fib->flags & FIB_CONTEXT_FLAG_NATIVE_HBA) {

			if (isFastResponse)
				fib->flags |= FIB_CONTEXT_FLAG_FASTRESP;

			if (fib->callback) {
				start_callback = 1;
			} else {
				unsigned long flagv;
				int completed = 0;

				dprintk((KERN_INFO ""event_wait up\n""));
				spin_lock_irqsave(&fib->event_lock, flagv);
				if (fib->done == 2) {
					fib->done = 1;
					completed = 1;
				} else {
					fib->done = 1;
					complete(&fib->event_wait);
				}
				spin_unlock_irqrestore(&fib->event_lock, flagv);

				spin_lock_irqsave(&dev->manage_lock, mflags);
				dev->management_fib_count--;
				spin_unlock_irqrestore(&dev->manage_lock,
					mflags);

				FIB_COUNTER_INCREMENT(aac_config.NativeRecved);
				if (completed)
					aac_fib_complete(fib);
			}
		} else {
			struct hw_fib *hwfib = fib->hw_fib_va;

			if (isFastResponse) {
				/* Doctor the fib */
				*(__le32 *)hwfib->data = cpu_to_le32(ST_OK);
				hwfib->header.XferState |=
					cpu_to_le32(AdapterProcessed);
				fib->flags |= FIB_CONTEXT_FLAG_FASTRESP;
			}

			if (hwfib->header.Command ==
				cpu_to_le16(NuFileSystem)) {
				__le32 *pstatus = (__le32 *)hwfib->data;

				if (*pstatus & cpu_to_le32(0xffff0000))
					*pstatus = cpu_to_le32(ST_OK);
			}
			if (hwfib->header.XferState &
				cpu_to_le32(NoResponseExpected | Async)) {
				if (hwfib->header.XferState & cpu_to_le32(
					NoResponseExpected)) {
					FIB_COUNTER_INCREMENT(
						aac_config.NoResponseRecved);
				} else {
					FIB_COUNTER_INCREMENT(
						aac_config.AsyncRecved);
				}
				start_callback = 1;
			} else {
				unsigned long flagv;
				int completed = 0;

				dprintk((KERN_INFO ""event_wait up\n""));
				spin_lock_irqsave(&fib->event_lock, flagv);
				if (fib->done == 2) {
					fib->done = 1;
					completed = 1;
				} else {
					fib->done = 1;
					complete(&fib->event_wait);
				}
				spin_unlock_irqrestore(&fib->event_lock, flagv);

				spin_lock_irqsave(&dev->manage_lock, mflags);
				dev->management_fib_count--;
				spin_unlock_irqrestore(&dev->manage_lock,
					mflags);

				FIB_COUNTER_INCREMENT(aac_config.NormalRecved);
				if (completed)
					aac_fib_complete(fib);
			}
		}


		if (start_callback) {
			/*
			 * NOTE:  we cannot touch the fib after this
			 *  call, because it may have been deallocated.
			 */
			if (likely(fib->callback && fib->callback_data)) {
				fib->callback(fib->callback_data, fib);
			} else {
				aac_fib_complete(fib);
				aac_fib_free(fib);
			}

		}
		return 0;
	}
}
"
